# ece49595nl-GloVe
This repository is purely focused on implementation

## Requirements

RocksDB

## Usage

## Procedure

1. Preprocess dataset into space-separated list of tokens
2. Generate vocab list with frequencies
3. Generate co-occurrences file
4. Train model

## Size and Space Complexity

```
L = average length of words
C = length of corpus in words
V = length of used vocabulary
R = number of rounds
```

### Time complexities

| Task                      | complexity    |
|          ---              |     ---       |
| Creating corpus           | $O(C * L)$    |
| Creating vocabulary       | $O(C)$        |
| Finding co-occurences     | $O(C)$        |
| Training model            | $O(R * V^2)$  |


### Space complexities- TODO

| Task                      | complexity    |
|          ---              |     ---       |
|                           |               |
|                           |               |
|                           |               |
|                           |               |


## Cost Function Details

### Cost function dimensionality

$
J = \sum\limits_{i,j=1}^{V} 
f(X_{i,j})
(w_i^T\tilde{w}_j + b_i + \tilde{b}_j - logX_{i,j})^2
$

With a vector length of 3, let's visualize each element.

$
J = \sum\limits_{i,j=1}^{V} 
f(X_{i,j}) (
\begin{bmatrix}
w_{i1} \\
w_{i2} \\
w_{i3}
\end{bmatrix}
\cdot
\begin{bmatrix}
\tilde{w}_{j1} & \tilde{w}_{j2} & \tilde{w}_{j3}
\end{bmatrix}
+b_i-\tilde{b}_i-logX_{i,j}
)^2
$

Where the dot product of $w_i^T\tilde{w}_j$ is 
$w_{i1}\tilde{w}_{j1} + w_{i2}\tilde{w}_{j2} + w_{i3}\tilde{w}_{j3}$

### Function f

$f$ is defined as a function of $X_{i,j}$ that weights frequencies in order to limit 
weighting of extremely frequent words.  

According to the paper

>1. f (0) = 0. If f is viewed as a continuous
function, it should vanish as x → 0 fast
enough that the limx→0 f (x) log2 x is finite.
>2. f (x) should be non-decreasing so that rare
co-occurrences are not overweighted.
>3. f (x) should be relatively small for large val-
ues of x, so that frequent co-occurrences are
not overweighted.

Also according to the paper, one working function is 

$f(x) =
\begin{cases} 
   (x / x_{max})^\alpha & \text{if } x < x_{max} \\
   1 & \text{otherwise}
  \end{cases}
$

Given a known $x_{max}$ and $0 < \alpha < 1$.

Our implementation will use this suggested function to begin with.

### Bias terms

## Implementation

### Tokenization

### Frequency generation

The frequencies are generated by reading in the file one word at a time using `std::ifstream` 
and counting frequencies using `std::map`.  This is written to an output text file by space 
separated pairs.

### Co-occurrence file generation

We decided to use a database to do this.


## Sources

[GloVe paper](https://aclanthology.org/D14-1162.pdf)
